{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christian Young<br>\n",
    "ML Assignment<br>\n",
    "26 March 2025<br>\n",
    "# Terms/Context\n",
    "- **NLQ** - Natural language query. This refers to the query that the user would provide e.g. \"How many players are there\"\n",
    "- **T2SQL** - Text-to-sql, when I say T2SQL I am referring to the model that I am using to convert a NL input to SQL\n",
    "- **Table of contents** - For convenience, each section has a link at the top to return to the table of contents\n",
    "\n",
    "# Summary\n",
    "### Major points of progress:\n",
    "**1. Dynamic calculation of relevant tables/columns**<br>\n",
    "_Issue_: The T2SQL model needs the relevant tables/models provided as context so that the model can generate correct SQL. This means if a user makes an NLQ, they would also have to specify which tables/columns they want to query (major inconvenience). With too many tables/columns provided as context, the model would not be able to correctly identify which tables/columns are the most relevant to the NLQ because of all the noise.<br><br>\n",
    "_Solution_: I implemented a Retrieval-Augmented Generation (RAG) architecture that dynamically calculates which tables/columns are relevant based on the NLQ. Now the user only has to provide their NQL. Granted, there is still fine-tuning that needs to be done, because there are queries that the model still struggles with.<br><br>\n",
    "**2. Fine-tuning model**<br>\n",
    "_Issue_: The T2SQL model struggled with some queries because the model was not trained specifically on our dataset<br><br>\n",
    "_Solution_: Leveraged parameter hypertuning to help fine-tune this model\n",
    "\n",
    "# Document overview\n",
    "\n",
    "[Section 1 Building rag architecture](#Section-1-rag-implementation)  \n",
    "&nbsp;&nbsp;- [Convert db schema and nlq to vector embeddings](#Convert-db-schema-and-nlq-to-vector-embeddings)  \n",
    "&nbsp;&nbsp;- [Perform similarity search](#Perform-similarity-search)  \n",
    "&nbsp;&nbsp;- [Convert search results to input prompt](#Convert-search-results-to-input-prompt)  \n",
    "\n",
    "[Section 2 Transfer learning](#Section-2-transfer-learning)  \n",
    "&nbsp;&nbsp;- [Test train split and data preprocessing](#Test-train-split-and-data-preprocessing)  \n",
    "&nbsp;&nbsp;- [Hyperparameter tuning and training](#Hyperparameter-tuning-and-training)\n",
    "\n",
    "[Section 3 Putting it all together](#Section-3-putting-it-all-together)\n",
    "\n",
    "[Section 4 Resources](#Section-4-resources)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chris\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "import sqlite3\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 rag implementation\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert db schema and nlq to vector embeddings\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding DB Schema**<br>\n",
    "In the excel file I have each data that includes column name, column description, and table name. This is all put in one string per column and that string is turned into a vector embedding. Each vector embedding has the same length<br><br>\n",
    "**Embedding NL query**<br>\n",
    "Similarly, each NLQ is being converted to vector embeddings of length 384. This will make similarity comparison easier.\n",
    "<br><br>\n",
    "\n",
    "In a production environment these would be stored in a vector database, but I keep them as variables in my script so that I can more quickly develop and prototype solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 DB columns:\n",
      "Table: Player,Column: id,Description: unique ID for each player. used to identify and count how many players\n",
      "Table: Player,Column: player_api_id,Description: API identifier for the player\n",
      "Table: Player,Column: player_name,Description: full name of the player (first and last name)\n",
      "Table: Player,Column: player_fifa_api_id,Description: unique FIFA-related identifier for the player\n",
      "Table: Player,Column: birthday,Description: birthdate of the player format\n",
      "Table: Player,Column: height,Description: height of the player in centimeters\n",
      "Table: Player,Column: weight,Description: weight of the player in kilograms\n",
      "Table: League,Column: id,Description: unique ID for each league. used to identify and count leagues\n",
      "Table: League,Column: country_id,Description: foreign key that links the league to a country\n",
      "Table: League,Column: name,Description: name of the league\n",
      "Table: Country,Column: id,Description: unique ID for each country. links to league's country_id\n",
      "Table: Country,Column: name,Description: name of the country\n",
      "\n",
      "DB schema shape: (12, 384)\n",
      "NL queries shape: (8, 384)\n"
     ]
    }
   ],
   "source": [
    "# list of columns with descriptions\n",
    "db_columns = pd.read_excel(\"../data/data_sheets.xlsx\", sheet_name=\"schema_metadata\")[\"full_column_metadata\"].tolist()\n",
    "\n",
    "# convert db metadata to vector embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "db_schema_embeddings = embedding_model.encode(db_columns)\n",
    "\n",
    "# convert NL queries to vector embedding\n",
    "nl_queries = [\n",
    "    \"Who is the tallest player?\",\n",
    "    \"Who is the heaviest player?\",\n",
    "    \"What is the name of the tallest player?\",\n",
    "    \"How many players are there?\",\n",
    "    \"How many countries are there\",\n",
    "    \"How many leagues are there?\",\n",
    "    \"What is the average player height?\",\n",
    "    \"What is the average player weight?\",\n",
    "]\n",
    "nl_queries_embeddings = embedding_model.encode(nl_queries)\n",
    "\n",
    "print(f\"All 12 DB columns:\")\n",
    "for col in db_columns:\n",
    "    print(col)\n",
    "print(f\"\\nDB schema shape: {db_schema_embeddings.shape}\")\n",
    "print(f\"NL queries shape: {nl_queries_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform similarity search\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Facebook AI Similarity Search**<br>\n",
    "This is an algorithm meant for finding semantic similarity between vectors. This means the model analyzes the actual meaning of the sentences rather than just comparing word similarity.\n",
    "\n",
    "**K parameter**<br>\n",
    "There is a parameter, K, that specifies how many search results will be returned. I talk about this later, but this is an opportunity for refinement, because ideally this should be dynamically generated based on the NLQ so that the model isn't feed too little/too much context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language query: Who is the tallest player?\n",
      "\n",
      "Search results:\n",
      "Table: Player,Column: height,Description: height of the player in centimeters\n",
      "Table: Player,Column: player_name,Description: full name of the player (first and last name)\n"
     ]
    }
   ],
   "source": [
    "# perform similarity search\n",
    "def search_similarity(num_of_search_results=4):\n",
    "    size_of_vectors = db_schema_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(size_of_vectors)\n",
    "    index.add(np.array(db_schema_embeddings))\n",
    "\n",
    "    D, I = index.search(np.array(nl_queries_embeddings), k=num_of_search_results)\n",
    "\n",
    "    all_search_results = []\n",
    "\n",
    "    for i in I:\n",
    "        matched_columns = []\n",
    "        for j in i:\n",
    "            matched_columns.append(db_columns[j])\n",
    "        all_search_results.append(matched_columns)\n",
    "\n",
    "    return all_search_results\n",
    "\n",
    "print(f\"Natural language query: {nl_queries[0]}\")\n",
    "print(f\"\\nSearch results:\")\n",
    "for column in search_similarity(num_of_search_results=2)[0]:\n",
    "    print(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert search results to input prompt\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing and reformatting search results**<br>\n",
    "The search algorithm gave the information that I needed - the code below is a lot of re-formatting. I need the input to the model to be formatted as a table creation statement along with the NLQ, so there was a lot of parsing and reformatting that I had to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def create_db_connection():\n",
    "    conn = sqlite3.connect(\"../eda/code/cyoung_eda.db\")\n",
    "    return conn, conn.cursor()\n",
    "\n",
    "def create_input_prompt(nl_query, relevant_context):\n",
    "    return f\"tables:\\n{relevant_context}\\nquery for: {nl_query}\"\n",
    "\n",
    "def execute_query(query, print_padding=None):\n",
    "    try:\n",
    "        conn, cursor = create_db_connection()\n",
    "        cursor.execute(query)\n",
    "\n",
    "        if (print_padding):\n",
    "            print(f\"{'Query results:':<{print_padding}}{cursor.fetchall()}\")\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.Error:\n",
    "        print(\"Error executing sql\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input prompt\n",
      "tables:\n",
      "CREATE TABLE Player (player_name TEXT,player_fifa_api_id INTEGER UNIQUE,height INTEGER,weight INTEGER)\n",
      "query for: What is the name of the tallest player?\n"
     ]
    }
   ],
   "source": [
    "# map search results to tables/columns\n",
    "def create_table_statement(relevant_columns):\n",
    "    tables_create_statements = {}\n",
    "    table_and_cols_search_results = {}\n",
    "    tables_in_search_results = list(set([result.split(\",\")[0].split(\":\")[1].strip() for result in relevant_columns]))\n",
    "    for table in tables_in_search_results:\n",
    "        table_columns = []\n",
    "        for result in relevant_columns:\n",
    "            if table == result.split(\",\")[0].split(\":\")[1].strip():\n",
    "                table_columns.append(result.split(\",\")[1].split(\":\")[1].strip())\n",
    "        if table in table_and_cols_search_results:\n",
    "            table_and_cols_search_results[table].extend(table_columns)\n",
    "        else:\n",
    "            table_and_cols_search_results[table] = table_columns\n",
    "        \n",
    "    # format schema portion of T2SQL model input\n",
    "    for table_name, columns in table_and_cols_search_results.items():\n",
    "        conn, cursor = create_db_connection()\n",
    "        cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name = '{table_name}'\")\n",
    "        raw_create_statement = cursor.fetchone()[0].encode().decode(\"unicode_escape\").replace(\"`\",\"\")\n",
    "        conn.close()\n",
    "        filtered_create_lines = []\n",
    "        create_lines = raw_create_statement.split(\"\\n\")\n",
    "        for line in create_lines:\n",
    "            line_segments = [segment.strip() for segment in line.split(\"\\t\")]\n",
    "            if (\"CREATE\" in line_segments[0] or \")\" == line_segments[0]):\n",
    "                pass\n",
    "            elif \"FOREIGN\" in line_segments[1]:\n",
    "                create_col_name = re.findall(r\"(?<=FOREIGN KEY\\()[\\w_]+(?=\\))\", line_segments[1])[0]\n",
    "                if any(search_col_name == create_col_name for search_col_name in columns):\n",
    "                    filtered_create_lines.append(create_col_name)\n",
    "            else:\n",
    "                create_col_name = line_segments[1]\n",
    "                create_col_details = line_segments[2]\n",
    "                if any(search_col_name == create_col_name for search_col_name in columns):\n",
    "                    filtered_create_lines.append(f\"{create_col_name} {create_col_details.replace(\",\", \"\")}\")\n",
    "\n",
    "        tables_create_statements[table_name] = f\"CREATE TABLE {table_name} ({\",\".join(filtered_create_lines)})\"\n",
    "    return \"\\n\".join(list(tables_create_statements.values()))\n",
    "\n",
    "creation_statements = []\n",
    "for search_result in search_similarity():\n",
    "    creation_statements.append(create_table_statement(search_result))\n",
    "\n",
    "input_prompts = []\n",
    "for i in range(len(nl_queries)):\n",
    "    input_prompts.append(\n",
    "        create_input_prompt(\n",
    "            nl_query=nl_queries[i],\n",
    "            relevant_context=creation_statements[i]))\n",
    "    \n",
    "print(f\"Example input prompt\\n{input_prompts[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 transfer learning\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/test split:**\n",
    "I used an Excel file to generate my training data. I did this because the formulas made it easy to build templates and drag to duplicate records based on the template. The input to the training data is the like the example you see above in the previous section, including the relevant tables/columns and the NLQ. The output to the training data is the SQL query. I did an 80/20 train/test split.\n",
    "<br><br>\n",
    "**Preprocessing:**\n",
    "I tokenize the data, and adjust parameters so that the data follows a unified format. For example, all inputs must be same length and all outputs must be same length. To achieve this, I pad when the value is too short, and also provide max length specifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test train split and data preprocessing\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc30c3aab4494889b428c85d7f7a5c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ce5fd5e73b47058fdb30e04751d1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql').to(device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('cssupport/t5-small-awesome-text-to-sql').to(device)\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "input_output_examples = pd.read_excel(\"../data/data_sheets.xlsx\", sheet_name=\"t2sql_data\")\n",
    "\n",
    "\n",
    "input_output_examples[\"search_results_formatted\"] = input_output_examples[\"search_results\"].fillna(\"\").apply(lambda s: create_table_statement(s.split(\";\")))\n",
    "\n",
    "input_output_examples[\"input\"] = input_output_examples.apply(\n",
    "    lambda row: create_input_prompt(row[\"natural_language_query\"], row[\"search_results_formatted\"]),\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(input_output_examples, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"input\", \"output\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"input\", \"output\"]])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\")\n",
    "  \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"output\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_TARGET_LENGTH\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning and training\n",
    "[Top](#Document-overview)<br>\n",
    "**Hyperparameter tuning**\n",
    "I chose to experiment with 3 parameters\n",
    "- Learning rate - how fast will learn. i.e. how big or small the adjustments will be\n",
    "- Per device train batch size - how many examples will the model learn from at a time\n",
    "- Weight decay - to avoid overfitting\n",
    "<br><br>\n",
    "\n",
    "**Training model**\n",
    "- I evaluate the training data against the test data so that it can make adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_7528\\2284337687.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=11.05084228515625, metrics={'train_runtime': 83.8352, 'train_samples_per_second': 0.573, 'train_steps_per_second': 0.072, 'total_flos': 6496406470656.0, 'train_loss': 11.05084228515625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"t5-base-medium-title-generation\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_name,  # where the model will be saved to\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "# https://huggingface.co/docs/transformers/en/hpo_train\n",
    "# hyperparameters\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "def model_init():\n",
    "    return T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction=[\"minimize\", \"maximize\"],\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 putting it all together\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Successes**<br>\n",
    "1. Dynamic context generation<br>\n",
    "You can see here that I am not manually providing the relevant tables/columns to the model as context - the RAG implementation dynamically generates this.\n",
    "2. Here I am using the fine-tuned model.\n",
    "\n",
    "**Opportunities for improvement**\n",
    "1. RAG refinements\n",
    "You will notice that many of these queries are incorrect. This is because the # of query results being returned from the similarity search algorithm in [section 2](#perform-similarity-search) is not dynamically set. Some NLQs have 4 relevant columns, others only have 2, and if I give them 4 the model will be confused because of the noise like what you see here. This means I need to explore ways to dynamically calculate how many relevant columns should be returned for an NLQ.\n",
    "2. Add more training data\n",
    "While I have assembled the full pipeline to train data, you can see that it did not make a major impact. This is only because I have ~60 rows of training data. Now I need to generate more so that it can be impactful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural language query:      Who is the tallest player?\n",
      "Generated SQL query:          SELECT player_name FROM Player WHERE weight = (SELECT player_fifa_api_id FROM Player)\n",
      "Query results:                []\n",
      "\n",
      "Natural language query:      Who is the heaviest player?\n",
      "Generated SQL query:          SELECT player_name FROM Player WHERE weight = (SELECT player_fifa_api_id FROM Player)\n",
      "Query results:                []\n",
      "\n",
      "Natural language query:      What is the name of the tallest player?\n",
      "Generated SQL query:          SELECT player_name FROM Player WHERE weight = (SELECT player_fifa_api_id FROM Player)\n",
      "Query results:                []\n",
      "\n",
      "Natural language query:      How many players are there?\n",
      "Generated SQL query:          SELECT COUNT(*) FROM Player\n",
      "Query results:                [(11060,)]\n",
      "\n",
      "Natural language query:      How many countries are there\n",
      "Generated SQL query:          SELECT COUNT(*) FROM Country WHERE country_id IN (SELECT country_id FROM Player)\n",
      "Error executing sql\n",
      "\n",
      "Natural language query:      How many leagues are there?\n",
      "Generated SQL query:          SELECT COUNT(*) FROM League\n",
      "Query results:                [(11,)]\n",
      "\n",
      "Natural language query:      What is the average player height?\n",
      "Generated SQL query:          SELECT AVG(weight) FROM Player\n",
      "Query results:                [(168.38028933092224,)]\n",
      "\n",
      "Natural language query:      What is the average player weight?\n",
      "Generated SQL query:          SELECT AVG(weight) FROM Player\n",
      "Query results:                [(168.38028933092224,)]\n"
     ]
    }
   ],
   "source": [
    "def generate_sql(input_prompt, use_fine_tuned_model=False):\n",
    "    # Load the tokenizer and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    # use fine-tuned model\n",
    "    if use_fine_tuned_model:\n",
    "        saved_model_path = \"t5-base-medium-title-generation/checkpoint-6/\"\n",
    "        model = T5ForConditionalGeneration.from_pretrained(saved_model_path).to(device)\n",
    "    else:\n",
    "        model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql').to(device)\n",
    "\n",
    "    \"\"\"Generate SQL query from natural language input.\"\"\"\n",
    "    inputs = tokenizer(input_prompt, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "padding = 30\n",
    "for i in range(len(input_prompts)):\n",
    "    sql_query = generate_sql(input_prompts[i])\n",
    "    print(f\"{'\\nNatural language query:':<{padding}}{nl_queries[i]}\")\n",
    "    print(f\"{'Generated SQL query:':<{padding}}{sql_query}\")\n",
    "    execute_query(sql_query, padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 resources\n",
    "[Top](#Document-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Fine tuning T5 model](https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887)\n",
    "- [Hyperparameters with hugging face](https://huggingface.co/docs/transformers/en/hpo_train)\n",
    "- [Text-to-sql model](https://huggingface.co/cssupport/t5-small-awesome-text-to-sql)\n",
    "- [Using Facebook AI Similarity Search (FAISS)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

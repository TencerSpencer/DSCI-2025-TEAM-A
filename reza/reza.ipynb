{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Assignment\n",
    "### Group A\n",
    "### 2/19/2025 \n",
    "**Christian Reza**\n",
    "\n",
    "Goal: \n",
    "* General schema description about an existing DB schema dataset (table/column count, relationships, etc.)\n",
    "* Created & populated DB schema\n",
    "* Learning points taken as we begin applying & fine-tuning T5 model to query DB schema (we do not expect to finish this, rather do technical exploration of a T5 model and how it can be fine-tuned)\n",
    "\n",
    "Using spider data from Kaggle, specifically the soccer_1 data: https://www.kaggle.com/datasets/jeromeblanchet/yale-universitys-spider-10-nlp-dataset/data\n",
    "* I will be focusing on the `Player` table and create the training data off the following rows:\n",
    "  * \"player_name\"\n",
    "  * \"birthday\"\n",
    "  * \"height\"\n",
    "  * \"weight\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages and pull TAPAS QA model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rza_t\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install tf-keras\n",
    "!pip install -q transformers\n",
    "!pip install -q setuptools\n",
    "!pip install -q tensorflow\n",
    "!pip install -q pandas\n",
    "import pandas as pd\n",
    "from transformers import TapasConfig, TapasTokenizer, TFTapasForQuestionAnswering\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFTapasForQuestionAnswering.\n",
      "\n",
      "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['aggregation_classifier', 'dropout_37', 'compute_token_logits', 'compute_column_logits']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# The base sized model with WTQ configuration\n",
    "model_name = \"google/tapas-base-finetuned-wtq\"\n",
    "config = TapasConfig.from_pretrained(model_name)\n",
    "model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data for TAPAS from Spider football data\n",
    "*Util script used supplied from tapas_utils*: https://github.com/NielsRogge/tapas_utils/blob/master/parse_answer_texts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q frozendict\n",
    "!pip install -q scipy\n",
    "!pip install -q numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# Lint as: python3\n",
    "\"\"\"This module implements a simple parser that can be used for TAPAS.\n",
    "\n",
    "Given a table, a question and one or more answer_texts, it will parse the texts\n",
    "to populate other fields (e.g. answer_coordinates, float_value) that are required\n",
    "by TAPAS.\n",
    "\n",
    "Please note that exceptions in this module are concise and not parameterized,\n",
    "since they are used as counter names in a BEAM pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import enum\n",
    "from typing import Callable, List, Text, Optional\n",
    "\n",
    "import six\n",
    "import struct\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import frozendict\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "class SupervisionMode(enum.Enum):\n",
    "  # Don't filter out any supervised information.\n",
    "  NONE = 0\n",
    "  # Remove all the supervised signals and recompute them by parsing answer\n",
    "  # texts.\n",
    "  REMOVE_ALL = 2\n",
    "  # Same as above but discard ambiguous examples\n",
    "  # (where an answer matches multiple cells).\n",
    "  REMOVE_ALL_STRICT = 3\n",
    "\n",
    "\n",
    "def _find_matching_coordinates(table, answer_text,\n",
    "                               normalize):\n",
    "  normalized_text = normalize(answer_text)\n",
    "  for row_index, row in table.iterrows():\n",
    "    for column_index, cell in enumerate(row):\n",
    "      if normalized_text == normalize(str(cell)):\n",
    "        yield (row_index, column_index)\n",
    "\n",
    "\n",
    "def _compute_cost_matrix_inner(\n",
    "    table,\n",
    "    answer_texts,\n",
    "    normalize,\n",
    "    discard_ambiguous_examples,\n",
    "):\n",
    "  \"\"\"Returns a cost matrix M where the value M[i,j] contains a matching cost from answer i to cell j.\n",
    "\n",
    "  The matrix is a binary matrix and -1 is used to indicate a possible match from\n",
    "  a given answer_texts to a specific cell table. The cost matrix can then be\n",
    "  usedto compute the optimal assignments that minimizes the cost using the\n",
    "  hungarian algorithm (see scipy.optimize.linear_sum_assignment).\n",
    "\n",
    "  Args:\n",
    "    table: a Pandas dataframe.\n",
    "    answer_texts: a list of strings.\n",
    "    normalize: a function that normalizes a string.\n",
    "    discard_ambiguous_examples: If true discard if answer has multiple matches.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if:\n",
    "      - we cannot correctly construct the cost matrix or the text-cell\n",
    "      assignment is ambiguous.\n",
    "      - we cannot find a matching cell for a given answer_text.\n",
    "\n",
    "  Returns:\n",
    "    A numpy matrix with shape (num_answer_texts, num_rows * num_columns).\n",
    "  \"\"\"\n",
    "  max_candidates = 0\n",
    "  n_rows, n_columns = table.shape[0], table.shape[1]\n",
    "  num_cells = n_rows * n_columns\n",
    "  num_candidates = np.zeros((n_rows, n_columns))\n",
    "  cost_matrix = np.zeros((len(answer_texts), num_cells))\n",
    "\n",
    "  for index, answer_text in enumerate(answer_texts):\n",
    "    found = 0\n",
    "    for row, column in _find_matching_coordinates(table, answer_text,\n",
    "                                                  normalize):\n",
    "      found += 1\n",
    "      cost_matrix[index, (row * len(table.columns)) + column] = -1\n",
    "      num_candidates[row, column] += 1\n",
    "      max_candidates = max(max_candidates, num_candidates[row, column])\n",
    "    if found == 0:\n",
    "      return None\n",
    "    if discard_ambiguous_examples and found > 1:\n",
    "      raise ValueError(\"Found multiple cells for answers\")\n",
    "\n",
    "  # TODO(piccinno): Shall we allow ambiguous assignments?\n",
    "  if max_candidates > 1:\n",
    "    raise ValueError(\"Assignment is ambiguous\")\n",
    "\n",
    "  return cost_matrix\n",
    "\n",
    "\n",
    "def _compute_cost_matrix(\n",
    "    table,\n",
    "    answer_texts,\n",
    "    discard_ambiguous_examples,\n",
    "):\n",
    "  \"\"\"Computes cost matrix.\"\"\"\n",
    "  for index, normalize_fn in enumerate(STRING_NORMALIZATIONS):\n",
    "    try:\n",
    "      result = _compute_cost_matrix_inner(\n",
    "          table,\n",
    "          answer_texts,\n",
    "          normalize_fn,\n",
    "          discard_ambiguous_examples,\n",
    "      )\n",
    "      if result is None:\n",
    "        continue\n",
    "      return result\n",
    "    except ValueError:\n",
    "      if index == len(STRING_NORMALIZATIONS) - 1:\n",
    "        raise\n",
    "  return None\n",
    "\n",
    "\n",
    "def _parse_answer_coordinates(table,\n",
    "                              answer_texts,\n",
    "                              discard_ambiguous_examples):\n",
    "  \"\"\"Populates answer_coordinates using answer_texts.\n",
    "\n",
    "  Args:\n",
    "    table: a Table message, needed to compute the answer coordinates.\n",
    "    answer_texts: a list of strings\n",
    "    discard_ambiguous_examples: If true discard if answer has multiple matches.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if the conversion fails.\n",
    "  \"\"\"\n",
    "  \n",
    "  cost_matrix = _compute_cost_matrix(\n",
    "      table,\n",
    "      answer_texts,\n",
    "      discard_ambiguous_examples,\n",
    "  )\n",
    "  if cost_matrix is None:\n",
    "    return\n",
    "  row_indices, column_indices = scipy.optimize.linear_sum_assignment(\n",
    "      cost_matrix)\n",
    " \n",
    "  # create answer coordinates as list of tuples\n",
    "  answer_coordinates = []\n",
    "  for row_index in row_indices:\n",
    "    flatten_position = column_indices[row_index]\n",
    "    row_coordinate = flatten_position // len(table.columns)\n",
    "    column_coordinate = flatten_position % len(table.columns)\n",
    "    answer_coordinates.append((row_coordinate, column_coordinate))\n",
    "\n",
    "  return answer_coordinates\n",
    "\n",
    "\n",
    "### START OF UTILITIES FROM TEXT_UTILS.PY ###\n",
    "\n",
    "def wtq_normalize(x):\n",
    "  \"\"\"Returns the normalized version of x.\n",
    "  This normalization function is taken from WikiTableQuestions github, hence the\n",
    "  wtq prefix. For more information, see\n",
    "  https://github.com/ppasupat/WikiTableQuestions/blob/master/evaluator.py\n",
    "  Args:\n",
    "    x: the object (integer type or string) to normalize.\n",
    "  Returns:\n",
    "    A normalized string.\n",
    "  \"\"\"\n",
    "  x = x if isinstance(x, six.text_type) else six.text_type(x)\n",
    "  # Remove diacritics.\n",
    "  x = \"\".join(\n",
    "      c for c in unicodedata.normalize(\"NFKD\", x)\n",
    "      if unicodedata.category(c) != \"Mn\")\n",
    "  # Normalize quotes and dashes.\n",
    "  x = re.sub(u\"[‘’´`]\", \"'\", x)\n",
    "  x = re.sub(u\"[“”]\", '\"', x)\n",
    "  x = re.sub(u\"[‐‑‒–—−]\", \"-\", x)\n",
    "  x = re.sub(u\"[‐]\", \"\", x)\n",
    "  while True:\n",
    "    old_x = x\n",
    "    # Remove citations.\n",
    "    x = re.sub(u\"((?\", \"\", x)\n",
    "  x = x.replace(\"\\n\", \" \")\n",
    "  return x\n",
    "\n",
    "\n",
    "_TOKENIZER = re.compile(r\"\\w+|[^\\w\\s]+\", re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize_string(x):\n",
    "  return list(_TOKENIZER.findall(x.lower()))\n",
    "\n",
    "\n",
    "# List of string normalization functions to be applied in order. We go from\n",
    "# simplest to more complex normalization procedures.\n",
    "STRING_NORMALIZATIONS = (\n",
    "    lambda x: x,\n",
    "    lambda x: x.lower(),\n",
    "    tokenize_string,\n",
    "    wtq_normalize,\n",
    ")\n",
    "\n",
    "\n",
    "def to_float32(v):\n",
    "  \"\"\"If v is a float reduce precision to that of a 32 bit float.\"\"\"\n",
    "  if not isinstance(v, float):\n",
    "    return v\n",
    "  return struct.unpack(\"!f\", struct.pack(\"!f\", v))[0]\n",
    "\n",
    "\n",
    "def convert_to_float(value):\n",
    "  \"\"\"Converts value to a float using a series of increasingly complex heuristics.\n",
    "  Args:\n",
    "    value: object that needs to be converted. Allowed types include\n",
    "      float/int/strings.\n",
    "  Returns:\n",
    "    A float interpretation of value.\n",
    "  Raises:\n",
    "    ValueError if the float conversion of value fails.\n",
    "  \"\"\"\n",
    "  if isinstance(value, float):\n",
    "    return value\n",
    "  if isinstance(value, int):\n",
    "    return float(value)\n",
    "  if not isinstance(value, six.string_types):\n",
    "    raise ValueError(\"Argument value is not a string. Can't parse it as float\")\n",
    "  sanitized = value\n",
    "\n",
    "  try:\n",
    "    # Example: 1,000.7\n",
    "    if \".\" in sanitized and \",\" in sanitized:\n",
    "      return float(sanitized.replace(\",\", \"\"))\n",
    "    # 1,000\n",
    "    if \",\" in sanitized and _split_thousands(\",\", sanitized):\n",
    "      return float(sanitized.replace(\",\", \"\"))\n",
    "    # 5,5556\n",
    "    if \",\" in sanitized and sanitized.count(\",\") == 1 and not _split_thousands(\n",
    "        \",\", sanitized):\n",
    "      return float(sanitized.replace(\",\", \".\"))\n",
    "    # 0.0.0.1\n",
    "    if sanitized.count(\".\") > 1:\n",
    "      return float(sanitized.replace(\".\", \"\"))\n",
    "    # 0,0,0,1\n",
    "    if sanitized.count(\",\") > 1:\n",
    "      return float(sanitized.replace(\",\", \"\"))\n",
    "    return float(sanitized)\n",
    "  except ValueError:\n",
    "    # Avoid adding the sanitized value in the error message.\n",
    "    raise ValueError(\"Unable to convert value to float\")\n",
    "\n",
    "### END OF UTILITIES FROM TEXT_UTILS.PY ###\n",
    "\n",
    "def _parse_answer_float(answer_texts, float_value):\n",
    "  if len(answer_texts) > 1:\n",
    "    raise ValueError(\"Cannot convert to multiple answers to single float\")\n",
    "  float_value = convert_to_float(answer_texts[0])\n",
    "  float_value = float_value\n",
    "\n",
    "  return answer_texts, float_value\n",
    "\n",
    "\n",
    "def _has_single_float_answer_equal_to(question, answer_texts, target):\n",
    "  \"\"\"Returns true if the question has a single answer whose value equals to target.\"\"\"\n",
    "  if len(answer_texts) != 1:\n",
    "    return False\n",
    "  try:\n",
    "    float_value = convert_to_float(answer_texts[0])\n",
    "    # In general answer_float is derived by applying the same conver_to_float\n",
    "    # function at interaction creation time, hence here we use exact match to\n",
    "    # avoid any false positive.\n",
    "    return to_float32(float_value) == to_float32(target)\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "\n",
    "def _parse_question(\n",
    "    table,\n",
    "    original_question,\n",
    "    answer_texts,\n",
    "    answer_coordinates,\n",
    "    float_value,\n",
    "    aggregation_function,\n",
    "    clear_fields,\n",
    "    discard_ambiguous_examples,\n",
    "):\n",
    "  \"\"\"Parses question's answer_texts fields to possibly populate additional fields.\n",
    "\n",
    "  Args:\n",
    "    table: a Pandas dataframe, needed to compute the answer coordinates.\n",
    "    original_question: a string.\n",
    "    answer_texts: a list of strings, serving as the answer to the question.\n",
    "    anser_coordinates:\n",
    "    float_value: a float, serves as float value signal. \n",
    "    aggregation_function: \n",
    "    clear_fields: A list of strings indicating which fields need to be cleared\n",
    "      and possibly repopulated.\n",
    "    discard_ambiguous_examples: If true, discard ambiguous examples.\n",
    "\n",
    "  Returns:\n",
    "    A Question message with answer_coordinates or float_value field populated.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if we cannot parse correctly the question message.\n",
    "  \"\"\"\n",
    "  question = original_question\n",
    "\n",
    "  # If we have a float value signal we just copy its string representation to\n",
    "  # the answer text (if multiple answers texts are present OR the answer text\n",
    "  # cannot be parsed to float OR the float value is different), after clearing\n",
    "  # this field.\n",
    "  if \"float_value\" in clear_fields and float_value is not None:\n",
    "    if not _has_single_float_answer_equal_to(question, answer_texts, float_value):\n",
    "      del answer_texts[:]\n",
    "      float_value = float(float_value)\n",
    "      if float_value.is_integer():\n",
    "        number_str = str(int(float_value))\n",
    "      else:\n",
    "        number_str = str(float_value)\n",
    "      answer_texts = []\n",
    "      answer_texts.append(number_str)\n",
    "\n",
    "  if not answer_texts:\n",
    "    raise ValueError(\"No answer_texts provided\")\n",
    "\n",
    "  for field_name in clear_fields:\n",
    "    if field_name == \"answer_coordinates\":\n",
    "        answer_coordinates = None\n",
    "    if field_name == \"float_value\":\n",
    "        float_value = None\n",
    "    if field_name == \"aggregation_function\":\n",
    "        aggregation_function = None\n",
    "\n",
    "  error_message = \"\"\n",
    "  if not answer_coordinates:\n",
    "    try:\n",
    "      answer_coordinates = _parse_answer_coordinates(\n",
    "          table,\n",
    "          answer_texts,\n",
    "          discard_ambiguous_examples,\n",
    "      )\n",
    "    except ValueError as exc:\n",
    "      error_message += \"[answer_coordinates: {}]\".format(str(exc))\n",
    "      if discard_ambiguous_examples:\n",
    "        raise ValueError(f\"Cannot parse answer: {error_message}\")\n",
    "\n",
    "  if not float_value:\n",
    "    try:\n",
    "      answer_texts, float_value = _parse_answer_float(answer_texts, float_value)\n",
    "    except ValueError as exc:\n",
    "      error_message += \"[float_value: {}]\".format(str(exc))\n",
    "\n",
    "  # Raises an exception if we cannot set any of the two fields.\n",
    "  if not answer_coordinates and not float_value:\n",
    "    raise ValueError(\"Cannot parse answer: {}\".format(error_message))\n",
    "\n",
    "  return question, answer_texts, answer_coordinates, float_value, aggregation_function\n",
    "\n",
    "\n",
    "# TODO(piccinno): Use some sort of introspection here to get the field names of\n",
    "# the proto.\n",
    "_CLEAR_FIELDS = frozendict.frozendict({\n",
    "    SupervisionMode.REMOVE_ALL: [\n",
    "        \"answer_coordinates\", \"float_value\", \"aggregation_function\"\n",
    "    ],\n",
    "    SupervisionMode.REMOVE_ALL_STRICT: [\n",
    "        \"answer_coordinates\", \"float_value\", \"aggregation_function\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "def parse_question(table, question, answer_texts, answer_coordinates=None, float_value=None, aggregation_function=None,\n",
    "                    mode=SupervisionMode.REMOVE_ALL):\n",
    "    \"\"\"Parses answer_text field of a question to populate additional fields required by TAPAS.\n",
    "\n",
    "    Args:\n",
    "        table: a Pandas dataframe, needed to compute the answer coordinates. Note that one should apply .astype(str)\n",
    "        before supplying the table to this function. \n",
    "        question: a string.\n",
    "        answer_texts: a list of strings, containing one or more answer texts that serve as answer to the question.\n",
    "        answer_coordinates: optional answer coordinates supervision signal, if you already have those. \n",
    "        float_value: optional float supervision signal, if you already have this. \n",
    "        aggregation_function: optional aggregation function supervised signal, if you already have this. \n",
    "        mode: see SupervisionMode enum for more information.\n",
    "\n",
    "    Returns:\n",
    "        A list with the question, populated answer_coordinates or float_value.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if we cannot parse correctly the question string.\n",
    "    \"\"\"\n",
    "    if mode == SupervisionMode.NONE:\n",
    "        return question, answer_texts\n",
    "\n",
    "    clear_fields = _CLEAR_FIELDS.get(mode, None)\n",
    "    if clear_fields is None:\n",
    "        raise ValueError(f\"Mode {mode.name} is not supported\")\n",
    "\n",
    "    return _parse_question(\n",
    "        table,\n",
    "        question,\n",
    "        answer_texts,\n",
    "        answer_coordinates,\n",
    "        float_value,\n",
    "        aggregation_function,\n",
    "        clear_fields,\n",
    "        discard_ambiguous_examples=mode == SupervisionMode.REMOVE_ALL_STRICT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Birthday</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Appindangoye</td>\n",
       "      <td>1992-02-29 00:00:00</td>\n",
       "      <td>187</td>\n",
       "      <td>182.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Cresswell</td>\n",
       "      <td>1989-12-15 00:00:00</td>\n",
       "      <td>146</td>\n",
       "      <td>170.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Doran</td>\n",
       "      <td>1991-05-13 00:00:00</td>\n",
       "      <td>163</td>\n",
       "      <td>170.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Galindo</td>\n",
       "      <td>1982-05-08 00:00:00</td>\n",
       "      <td>198</td>\n",
       "      <td>182.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Hughes</td>\n",
       "      <td>1979-11-08 00:00:00</td>\n",
       "      <td>154</td>\n",
       "      <td>182.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aaron Hunt</td>\n",
       "      <td>1986-09-04 00:00:00</td>\n",
       "      <td>161</td>\n",
       "      <td>182.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aaron Kuhl</td>\n",
       "      <td>1996-01-30 00:00:00</td>\n",
       "      <td>146</td>\n",
       "      <td>172.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Player Name             Birthday Weight  Height\n",
       "0  Aaron Appindangoye  1992-02-29 00:00:00    187  182.88\n",
       "1     Aaron Cresswell  1989-12-15 00:00:00    146  170.18\n",
       "2         Aaron Doran  1991-05-13 00:00:00    163  170.18\n",
       "3       Aaron Galindo  1982-05-08 00:00:00    198  182.88\n",
       "4        Aaron Hughes  1979-11-08 00:00:00    154  182.88\n",
       "5          Aaron Hunt  1986-09-04 00:00:00    161  182.88\n",
       "6          Aaron Kuhl  1996-01-30 00:00:00    146  172.72"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"Player Name\": [\n",
    "        \"Aaron Appindangoye\",\n",
    "        \"Aaron Cresswell\",\n",
    "        \"Aaron Doran\",\n",
    "        \"Aaron Galindo\",\n",
    "        \"Aaron Hughes\",\n",
    "        \"Aaron Hunt\",\n",
    "        \"Aaron Kuhl\",\n",
    "    ],\n",
    "    \"Birthday\": [\n",
    "        \"1992-02-29 00:00:00\",\n",
    "        \"1989-12-15 00:00:00\",\n",
    "        \"1991-05-13 00:00:00\",\n",
    "        \"1982-05-08 00:00:00\",\n",
    "        \"1979-11-08 00:00:00\",\n",
    "        \"1986-09-04 00:00:00\",\n",
    "        \"1996-01-30 00:00:00\",\n",
    "    ],\n",
    "    \"Weight\": [\n",
    "        \"187\",\n",
    "        \"146\",\n",
    "        \"163\",\n",
    "        \"198\",\n",
    "        \"154\",\n",
    "        \"161\",\n",
    "        \"146\",\n",
    "    ],\n",
    "    \"Height\": [\n",
    "        \"182.88\",\n",
    "        \"170.18\",\n",
    "        \"170.18\",\n",
    "        \"182.88\",\n",
    "        \"182.88\",\n",
    "        \"182.88\",\n",
    "        \"172.72\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.astype(str)\n",
    "\n",
    "table.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much does Aaron Hughes weigh?\n",
      "['154']\n",
      "Found coordinates: [(np.int64(4), np.int64(2))]\n",
      "Found float value: 154.0\n"
     ]
    }
   ],
   "source": [
    "question = \"How much does Aaron Hughes weigh?\"\n",
    "\n",
    "answer_texts = [\"154\"]\n",
    "\n",
    "question, answer_texts, answer_coordinates, float_value, aggregation_function = parse_question(table=table, question=question, answer_texts=answer_texts)\n",
    "print(question)\n",
    "print(answer_texts)\n",
    "print(\"Found coordinates:\", answer_coordinates)\n",
    "print(\"Found float value:\", float_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Aaron Appindangoye's birthday?\n",
      "['1992-02-29 00:00:00']\n",
      "Found coordinates: [(np.int64(0), np.int64(1))]\n",
      "Found float value: None\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Aaron Appindangoye's birthday?\"\n",
    "\n",
    "answer_texts = [\"1992-02-29 00:00:00\"]\n",
    "\n",
    "question, answer_texts, answer_coordinates, float_value, aggregation_function = parse_question(table=table, question=question, answer_texts=answer_texts)\n",
    "print(question)\n",
    "print(answer_texts)\n",
    "print(\"Found coordinates:\", answer_coordinates)\n",
    "print(\"Found float value:\", float_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many players weigh more than 150?\n",
      "['187', '163', '198', '154', '161']\n",
      "Found coordinates: [(np.int64(0), np.int64(2)), (np.int64(2), np.int64(2)), (np.int64(3), np.int64(2)), (np.int64(4), np.int64(2)), (np.int64(5), np.int64(2))]\n",
      "Found float value: None\n"
     ]
    }
   ],
   "source": [
    "question = \"How many players weigh more than 150?\"\n",
    "\n",
    "answer_texts = [\"187\", \"163\", \"198\", \"154\", \"161\"]\n",
    "float_value = 4\n",
    "\n",
    "question, answer_texts, answer_coordinates, float_value, aggregation_function = parse_question(table=table, question=question, answer_texts=answer_texts)\n",
    "print(question)\n",
    "print(answer_texts)\n",
    "print(\"Found coordinates:\", answer_coordinates)\n",
    "print(\"Found float value:\", float_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take output from SQA Util script to create tensor data to fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       "array([[ 101, 2129, 2172, ...,    0,    0,    0],\n",
       "       [ 101, 2054, 2003, ...,    0,    0,    0],\n",
       "       [ 101, 2129, 2116, ...,    0,    0,    0]], dtype=int32)>, 'labels': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'numeric_values': <tf.Tensor: shape=(3, 512), dtype=float32, numpy=\n",
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>, 'numeric_values_scale': <tf.Tensor: shape=(3, 512), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>, 'token_type_ids': <tf.Tensor: shape=(3, 512, 7), dtype=int32, numpy=\n",
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = TapasTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data = {\n",
    "    \"Player Name\": [\n",
    "        \"Aaron Appindangoye\",\n",
    "        \"Aaron Cresswell\",\n",
    "        \"Aaron Doran\",\n",
    "        \"Aaron Galindo\",\n",
    "        \"Aaron Hughes\",\n",
    "        \"Aaron Hunt\",\n",
    "        \"Aaron Kuhl\",\n",
    "    ],\n",
    "    \"Birthday\": [\n",
    "        \"1992-02-29 00:00:00\",\n",
    "        \"1989-12-15 00:00:00\",\n",
    "        \"1991-05-13 00:00:00\",\n",
    "        \"1982-05-08 00:00:00\",\n",
    "        \"1979-11-08 00:00:00\",\n",
    "        \"1986-09-04 00:00:00\",\n",
    "        \"1996-01-30 00:00:00\",\n",
    "    ],\n",
    "    \"Weight\": [\n",
    "        \"187\",\n",
    "        \"146\",\n",
    "        \"163\",\n",
    "        \"198\",\n",
    "        \"154\",\n",
    "        \"161\",\n",
    "        \"146\",\n",
    "    ],\n",
    "    \"Height\": [\n",
    "        \"182.88\",\n",
    "        \"170.18\",\n",
    "        \"170.18\",\n",
    "        \"182.88\",\n",
    "        \"182.88\",\n",
    "        \"182.88\",\n",
    "        \"172.72\",\n",
    "    ],\n",
    "}\n",
    "queries = [\n",
    "    \"How much does Aaron Hughes weigh?\",\n",
    "    \"What is Aaron Appindangoye's birthday?\",\n",
    "    \"How many players weigh more than 150?\",\n",
    "]\n",
    "answer_coordinates = [[(4, 2)], [(0, 1)], [(0, 2), (2, 2), (3, 2), (4, 2), (5, 2)]]\n",
    "answer_text = [[\"154\"], [\"1992-02-29 00:00:00\"], [\"4\"]]\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "inputs = tokenizer(\n",
    "    table=table,\n",
    "    queries=queries,\n",
    "    answer_coordinates=answer_coordinates,\n",
    "    answer_text=answer_text,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"tf\",\n",
    ")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sqa_path = Path(\"C:/School/Spring 2025/DSCI-2025-TEAM-A/reza/data/sqa_train_set.csv\")\n",
    "table_csv_path = Path(\"C:/School/Spring 2025/DSCI-2025-TEAM-A/reza/data/\")\n",
    "\n",
    "\n",
    "class TableDataset:\n",
    "    \"\"\"\n",
    "    A dataset class for handling table data and tokenizing it for model input.\n",
    "    Attributes:\n",
    "        data (pd.DataFrame): The input data containing table file paths, questions, and answers.\n",
    "        tokenizer (Tokenizer): The tokenizer used to process the table data and questions.\n",
    "    Methods:\n",
    "        __iter__():\n",
    "            Iterates over the dataset, tokenizes the table data and questions, and yields the tokenized inputs.\n",
    "        __len__():\n",
    "            Returns the length of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx in range(self.__len__()):\n",
    "            item = self.data.iloc[idx]\n",
    "            table = pd.read_csv(str(table_csv_path) + os.sep + item.table_file).astype(\n",
    "                str\n",
    "            )  # be sure to make your table data text only\n",
    "            encoding = self.tokenizer(\n",
    "                table=table,\n",
    "                queries=item.question,\n",
    "                answer_coordinates=eval(item.answer_coordinates),\n",
    "                # answer_coordinates=item.answer_coordinates,\n",
    "                answer_text=item.answer_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"tf\",\n",
    "            )\n",
    "            # remove the batch dimension which the tokenizer adds by default\n",
    "            encoding = {key: tf.squeeze(val, 0) for key, val in encoding.items()}\n",
    "            # add the float_answer which is also required (weak supervision for aggregation case)\n",
    "            float_answer = item.float_answer if not pd.isna(item.float_answer) else 0.0\n",
    "            encoding[\"float_answer\"] = tf.convert_to_tensor(\n",
    "                item.float_answer, dtype=tf.float32\n",
    "            )\n",
    "            yield encoding[\"input_ids\"], encoding[\"attention_mask\"], encoding[\n",
    "                \"numeric_values\"\n",
    "            ], encoding[\"numeric_values_scale\"], encoding[\"token_type_ids\"], encoding[\n",
    "                \"labels\"\n",
    "            ], encoding[\n",
    "                \"float_answer\"\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "data = pd.read_csv(sqa_path, sep=\"\\t\")\n",
    "train_dataset = TableDataset(data, tokenizer)\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(512,), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(512,), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(512, 7), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    ")\n",
    "train_dataloader = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(train_dataset), output_signature=output_signature\n",
    ").batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the TAPAS model with new data!\n",
    "* This I am confused on, what was the purpose of creating the `inputs` from the dataframe, do we still need the tsv to train the data?\n",
    "* What is wrong with my `train_dataloader` that is causing an error in the batches - is there something I missed in the data loader, do I need more training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "C:\\Users\\rza_t\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFTapasEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1,427] = 10582 is not in [0, 256) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFTapasEmbeddings):\n  • input_ids=tf.Tensor(shape=(3, 512), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(3, 512, 7), dtype=int32)\n  • inputs_embeds=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_values_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_values_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(outputs\u001b[38;5;241m.\u001b[39mloss, model\u001b[38;5;241m.\u001b[39mtrainable_weights)\n\u001b[0;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_weights))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\modeling_tf_tapas.py:1465\u001b[0m, in \u001b[0;36mTFTapasForQuestionAnswering.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, table_mask, aggregation_labels, float_answer, numeric_values, numeric_values_scale, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(TAPAS_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mTFTableQuestionAnsweringOutput, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1412\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFTableQuestionAnsweringOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;124;03m    table_mask (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;124;03m        Mask for the table. Indicates which tokens belong to the table (1). Question tokens, table headers and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    >>> logits_aggregation = outputs.logits_aggregation\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1465\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtapas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1479\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\modeling_tf_tapas.py:888\u001b[0m, in \u001b[0;36mTFTapasMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtype_vocab_sizes)], value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(attention_mask, (input_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\tapas\\modeling_tf_tapas.py:226\u001b[0m, in \u001b[0;36mTFTapasEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, training)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_token_type_embeddings):\n\u001b[0;32m    225\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_embeddings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 226\u001b[0m     final_embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings)\n\u001b[0;32m    229\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFTapasEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1,427] = 10582 is not in [0, 256) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFTapasEmbeddings):\n  • input_ids=tf.Tensor(shape=(3, 512), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(3, 512, 7), dtype=int32)\n  • inputs_embeds=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# this is the default WTQ configuration\n",
    "config = TapasConfig(\n",
    "    num_aggregation_labels=4,\n",
    "    use_answer_as_supervision=True,\n",
    "    answer_loss_cutoff=0.664694,\n",
    "    cell_selection_preference=0.207951,\n",
    "    huber_loss_delta=0.121194,\n",
    "    init_cell_selection_weights_to_zero=True,\n",
    "    select_one_column=True,\n",
    "    allow_empty_column_selection=False,\n",
    "    temperature=0.0352513,\n",
    ")\n",
    "# model = TFTapasForQuestionAnswering.from_pretrained(model_name, config=config)\n",
    "# model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\", config=config)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    for batch in train_dataloader:\n",
    "        # get the inputs;\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        token_type_ids = batch[4]\n",
    "        labels = batch[-1]\n",
    "        numeric_values = batch[2]\n",
    "        numeric_values_scale = batch[3]\n",
    "        float_answer = batch[6]\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels,\n",
    "                numeric_values=numeric_values,\n",
    "                numeric_values_scale=numeric_values_scale,\n",
    "                float_answer=float_answer,\n",
    "            )\n",
    "        grads = tape.gradient(outputs.loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the newly fine-tune model against soccer dataset!\n",
    "***TBD***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
